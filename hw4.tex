\documentclass[addpoints]{exam}
\usepackage{url}
\usepackage{amsmath,amsthm,enumitem}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{qtree}
\usepackage[nodayofweek,level]{datetime}
\usepackage{color}
\usepackage{csquotes}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}
\usepackage{algorithm,algpseudocode}
\usepackage{graphicx}
\usetikzlibrary{positioning}
\usetikzlibrary{calc,arrows.meta,positioning}

\tikzset{
    every node/.style={font=\sffamily\small},
    main node/.style={thick,circle,draw,font=\sffamily\Large}
}
\newtheorem*{claim}{Claim}
\definecolor{qcolor}{rgb}{0, 0, 0.3}
\definecolor{acolor}{rgb}{0, 0, 0}
%\input myfonts
\qformat{Question \thequestion: \thequestiontitle\dotfill \textbf{[\totalpoints]}}
\pointname{}
\bonuspointname{}
\pointformat{[\bfseries\thepoints]}

\lhead{Gopal Menon (u0772360)}
\chead{\bf{HW4}}
\rhead{CS 6150 \today}
\headrule

\begin{document}

\section*{Collaborators}

Ben Nelson, Corbin Baldwin and I collaborated for this assignment.

\begin{questions}
\titledquestion{Balls and bins}
Consider the process we saw in class, of distributing $n$ balls into $n$ bins, independently and uniformly at random.
\begin{parts}
\part[4] What is the expected number of empty bins?
\part[2] Show that the probability that $80\%$ of the bins are empty is $\le 1/2$. 
\part[2] Let $X_i$ be the random variable that is $1$ if bin $i$ is empty and is $0$ otherwise.  Are $X_1$ and $X_2$ independent? Give an intuitive reasoning.
\part[4] Let $Z$ be the random variable defined by $Z = \sum_i X_i$ (i.e., the number of empty bins). Suppose you are told that the variance of $Z$ is $\le n$.  Use this to prove that the probability that $80\%$ of the bins are empty is $< 8/n$.  [Note that this is a much better bound than the one in part (b) when $n$ is large.]
\end{parts}

\begin{parts}

\part 
\begin{equation*}
\begin{aligned}
    \text{Let random variable } X_i &= 
\begin{dcases}
    1,& \text{when slot } i \text{ is empty}\\
    0,              & \text{otherwise}
\end{dcases}\\
\text{Number of empty bins} &= \sum_{i=1}^n X_i\\
\mathbb{E} \left [ \text{Number of empty bins} \right ] &= \mathbb{E} \left [ \sum_{i=1}^n X_i \right ] \\
&= \sum_{i=1}^n \mathbb{E} \left [ X_i \right ] \text{, due to linearity of expectation}\\
&= \sum_{i=1}^n \left ( 1 \times \Pr \left [ X_i =1 \right ]  + 0 \times \Pr \left [ X_i =0 \right ] \right )\\
&= \sum_{i=1}^n \Pr \left [ X_i =1 \right ] \\ 
&= \sum_{i=1}^n \frac{(n-1)^n}{n^n} \\
\text{since } n \text{ balls can be put into } &n-1 \text{ slots in } (n-1)^n \text{ ways}\\
\text{ and } n \text{ balls can be put into } &n \text{ slots in } n^n \text{different ways}\\
&= n \frac{(n-1)^n}{n^n} \\
&= \frac{(n-1)^n}{n^{n-1}}
\end{aligned}
\end{equation*}

\part \begin{equation*}
\begin{aligned}
\Pr \left [80\% \text{ of the bins are empty} \right ] &= \Pr \left [ \text{Number of empty bins } \geq 0.8n\right ]\\
&=  \Pr \left [ X_e \geq 0.8n\right ] \text{, where } X_e = \text{ number of empty bins}\\
&=  \Pr \left [ X_e \geq \frac{0.8n}{\mathbb{E} \left [X_e \right ]}\mathbb{E} \left [X_e \right ] \right ]\\
&\leq \frac{\mathbb{E} \left [X_e \right ]}{0.8n} \text{, by Markov's Inequality}\\
&= \frac{(n-1)^n}{n^{n-1}}\frac{1}{0.8n}\\
&= \frac{1.25 (n-1)^n}{n^n}\\
\text{Let } y &= \frac{1.25 (n-1)^n}{n^n}\\
&= 1.25 (n-1)^n n^{-n}\\
\frac{dy}{dn} &= 1.25 n (n-1)^{n-1} n^{-n} -1.25 n (n-1)^n n^{-n-1}\\
&= 1.25 n (n-1)^{n-1} n^{-n} \left ( 1 - (n-1) n^{-1}\right )\\
&= 1.25 n (n-1)^{n-1} n^{-n} \left (1 - \left (1-\frac{1}{n} \right ) \right )\\
&= 1.25 n (n-1)^{n-1} n^{-n} \frac{1}{n} \\
&\geq 0 \text{, for } n > 1\\
 \lim_{n \to \infty} \frac{1.25 (n-1)^n}{n^n} &=  \lim_{n \to \infty} 1.25 \left ( \frac{n-1}{n} \right ) ^n\\
 &= 1.25 \lim_{n \to \infty} \left (1 - \frac{1}{n} \right ) ^n\\
&= 1.25 \frac{1}{e} \\
&= 0.4598\\
&< \frac{1}{2}
\end{aligned}
\end{equation*}

Since $\frac{dy}{dn} > 0$ for $n>1$ and $\lim_{n \to \infty} y < \frac{1}{2}$, $\Pr \left [80\% \text{ of the bins are empty} \right ] <  \frac{1}{2}$.

\part If bin 1 is known to be empty, then there is a higher chance that bin 2 is not empty, since all of the $n$ balls would have been assigned to the $n-1$ remaining buckets. This means that $X_1$ and $X_2$ are not independent random variables.

\part \begin{equation*}
\begin{aligned}
\mathbb{E} \left [ Z \right ] &= \frac{(n-1)^n}{n^{n-1}} \text{, from part a above}\\
&= n \frac{(n-1)^n}{n^n}\\
&= n \left ( 1-\frac{1}{n} \right ) ^n\\
&< \frac{n}{e} \text{, since } \lim_{n \to \infty} \left ( 1-\frac{1}{n} \right ) ^n = \frac{1}{e}\\
\Pr \left [80\% \text{ of the bins are empty} \right ] &= \Pr \left [ \text{Number of empty bins } \geq 0.8n\right ]\\
&=  \Pr \left [ Z \geq 0.8n\right ] \\
&= \Pr \left [ Z - \mathbb{E} \left [ Z \right ] \geq 0.8n - \mathbb{E} \left [ Z \right ] \right ] \\
&= \Pr \left [ Z - \mathbb{E} \left [ Z \right ] \geq \frac{0.8n - \mathbb{E} \left [ Z \right ]}{\sigma} \sigma \right ] \text{, where } \sigma^2 \text{ is the variance of } Z\\
&\leq \frac{1}{\left (\frac{0.8n - \mathbb{E} \left [ Z \right ]}{\sigma} \right )^2}   \text{ , by Chebychev's inequality}\\
&= \frac{\sigma^2}{\left ( 0.8n - \mathbb{E} \left [ Z \right ] \right ) ^2}\\
&\leq \frac{n}{\left ( 0.8n - \mathbb{E} \left [ Z \right ] \right ) ^2}\\
&\leq \frac{n}{\left ( 0.8n - \frac{n}{e} \right) ^2}\\
&= \frac{ne^2}{n^2\left ( 0.8e - 1 \right)^2}\\
&= \frac{1}{n}\frac{e^2}{\left ( 0.8e - 1 \right)^2}\\
&= \frac{5.355}{n}\\
&< \frac{8}{n} \qed
\end{aligned}
\end{equation*}

\end{parts}

\titledquestion{Median/order finding}
Consider the following simple algorithm for finding the $k$th smallest element in a given unsorted array $A[1], A[2], \dots, A[n]$ (assume that all the $A[i]$ are distinct):
\begin{enumerate}
\item pick a uniformly random index $i \in \{1, 2, \dots, n\}$
\item divide the array $A[]$ into two sub-arrays -- $B[]$ and $C[]$, where the elements of $B[]$ are all $\le A[i]$ and the elements of $C[]$ are $> A[i]$. 
\item if length$(B) \geq k$, we recursively find the $k$th smallest element in $B[]$; else we find the $k - \text{length}(B)$'th smallest element in $C[]$.
\end{enumerate}

\begin{parts}
\part[2] Prove that the running time on the array $A$ of length $n$ can be bounded as
\[ T(n)  \le \max\{ T(\text{length}(B)), T(\text{length}(C)) \} + O(n). \]
\part[3] Give an input $A, k$, and an unlucky choice of indices $i$ that leads to a running time larger than $n^2/4$.
\part[5] Let $f(n)$ denote the {\em expected running time} on an input array of length $n$.  Derive a ``probabilistic recurrence'' analogous to the one we saw in class for quicksort, and show that $f(n) = O(n)$. [{\em Hint:} use part (a).]
\end{parts}

\begin{parts}

\part To divide the array into two sub-arrays, we would need to compare each element with the one corresponding to the position of the randomly chosen index. This will take $O(n)$ time. In the worst case, the element corresponding to the $k^{th}$ smallest element in the original array, would lie in the larger of the two sub-arrays. This means that the running time on the array $A$ of length $n$ can be bounded as $T(n)  \le \max\{ T(\text{length}(B)), T(\text{length}(C)) \} + O(n)$.

\part Let $k = 7$ and $A = \{6,2,5,4,3,1,9\}$ \\
$B=\{1\}$ and $C=\{6,2,5,4,3,9\}$ when $i=6$\\
Since length(B)$=1<k$, we need to find the $7-1=6^{th}$ smallest element in $C$\\ 
$\{2\}$ and $\{6,5,4,3,9\}$ are the two sub-arrays when $i=2$\\
Since length(first sub-array)$=1<6$, we need to find the $6-1=5^{th}$ smallest element in the second sub-array\\ 
$\{3\}$ and $\{6,5,4,9\}$ are the two sub-arrays when $i=4$\\
Since length(first sub-array)$=1<5$, we need to find the $5-1=4^{th}$ smallest element in the second sub-array\\ 
$\{4\}$ and $\{6,5,9\}$ are the two sub-arrays when $i=3$\\
Since length(first sub-array)$=1<4$, we need to find the $4-1=3^{rd}$ smallest element in the second sub-array\\ 
$\{5\}$ and $\{6,9\}$ are the two sub-arrays when $i=2$\\
Since length(first sub-array)$=1<3$, we need to find the $3-1=2^{nd}$ smallest element in the second sub-array\\ 
$\{6\}$ and $\{9\}$ are the two sub-arrays when $i=1$\\
Since length(first sub-array)$=1<2$, we need to find the $2-1=1^{st}$ smallest element in the second sub-array\\ 
We get $9$ as the answer, which is the $k = 7^{th}$ smallest element in the original array $A = \{6,2,5,4,3,1,9\}$\\

If we add up the lengths of the arrays worked on, we get $7+6+5+4+3+2=27>\frac{n^2}{4}=\frac{7^2}{4}=\frac{49}{4}=12.25$.

\part Consider the base case of an array of length $1$. $T(1)=O(1)$ since it takes constant time to find the $1^{st}$ smallest element in the array. Since the recurrence is bounded as 
\[ T(n)  \le \max\{ T(\text{length}(B)), T(\text{length}(C)) \} + O(n). \]

Consider the case where array $B$ has a length of $n-r$ and array $C$ has a length on $r$. The recurrence will be
\[ T(n)  \le \max\{ T(n-r), T(r) \} + O(n). \]

\begin{equation*}
\begin{aligned}
\mathbb{E} \left [ T(n) \right ] & \leq \sum_{r=0}^n \Pr \left [ r \right ] \left (\max\{ T(n-r), T(r) \} + O(n) \right ), \text{(} r \text{ starts at } 0 \text{ when largest element is chosen)}\\
&= \sum_{r=0}^n \frac{1}{n} \left (\max\{ T(n-r), T(r) \} + O(n) \right ) \text{, since the probability of any } r \text{ is } \frac{1}{n}\\
&= \sum_{r=0}^n \frac{1}{n} \left (\max\{ O(n-r), O(r) \} + O(n) \right ) \text{, assuming }T(n-r) = O(n-r) \text{ and } T(r)=O(r)\\
&= \sum_{r=0}^n \frac{1}{n} \left (\max\{(n-r), r \} + n \right )\text{, taking the constant to be } 1\\
&= \frac{1}{n} \left (2\left(\frac{n(n+1)}{2} - \frac{\frac{n}{2}(\frac{n}{2}+1)}{2} \right ) + n^2 \right ) \text{, as we take the larger sub-array for worst case}\\
&= \frac{1}{n} \left ( n^2 + n - \frac{n^2}{4} -\frac{n}{2} + n^2 \right )\\
&=\frac{1}{n} \left (\frac{7}{4} n^2  + \frac{n}{2} \right )\\
&= \frac{7}{4} n + \frac{1}{2} \\
&= O(n) \qed
\end{aligned}
\end{equation*}

\end{parts}

\titledquestion{Quicksort revisited}
Recall the quicksort procedure we saw in class, where we pick a random element of the input array as the {\em pivot}, recursively sort the elements less than and larger than the pivot, and concatenate the solutions.  We saw that the expected running time is $O(n \log n)$.  

\begin{parts}
\part[2] Using a basic implementation (base case being a singleton), find the {\em constant} in the $O()$ notation for the algorithm above. (You may do this by picking any array, repeatedly running the procedure above, and averaging the values of running time divided by $n \log n$.)
\part[2]  Now, consider the following procedure: for $k=1, 2, 3$, first pick $(2k+1)$ random indices, and choose their {\em median} as the pivot. Now report the constant in the $O()$ notation. 
\part[2] Explain your observations intuitively.
\end{parts}

\begin{parts}

\part The constant that was computed for various random integer array sizes for runtimes averaged over $20$ iterations is shown below
\begin{center}
  \begin{tabular}{ |c |c|}
    \hline
    Array Size &  Constant\\ \hline
   $10^5$& $5.21\times 10^{-6}$  \\ \hline
   $10^6$& $3.47\times 10^{-6}$  \\ \hline
   $10^7$& $2.74\times 10^{-6}$  \\ \hline
   $10^8$& $2.69\times 10^{-6}$  \\ \hline
  \end{tabular}
\end{center}

\part The value of the constant for various array sizes is shown below

\begin{center}
  \begin{tabular}{ |c |c|c|c|}
    \hline
    Array Size &  $k=1$ & $k=2$ & $k=3$\\ \hline
   $10^5$ & $6.08\times 10^{-6}$  & $6.08\times 10^{-6}$ & $6.95\times 10^{-6} $\\ \hline
   $10^6$ & $4.85\times 10^{-6}$  & $5.14\times 10^{-6} $&   $5.57\times 10^{-6} $ \\ \hline
   $10^7$ & $4.20\times 10^{-6}$  & $4.52\times 10^{-6} $& $4.98\times 10^{-6} $   \\ \hline
   $10^8$ & $3.63\times 10^{-6}$  & $4.17\times 10^{-6} $&   $4.43\times 10^{-6} $ \\ \hline
  \end{tabular}
\end{center}

\part The value of the constant is least for the case of the single random pivot and higher for the cases where a median of random elements is used as a pivot. It looks like the work involved in computing the median of elements negated the expected boost in performance from choosing a supposedly better pivot. The more the elements that a pivot is chosen from, the higher the value of the constant and the run time. The best performance was from choosing a single random pivot as it involved minimum extra work and performance was good as it was improbable that many pivots would be bad and adversely impact run time.

\end{parts}

\titledquestion{Checking matrix multiplication}
Matrix multiplication is one of the classic algorithmic problems.  Consider the problem of multiplying two $n \times n$ matrices over the field $\mathbf{F}_2$ (i.e., we have matrices with entries $0/1$, and we perform all computations modulo $2$; e.g., 0*0 + 1*1 + 1*1 + 1*0 = 0).

The best known algorithms here are messy and take time $O(n^{2.36...})$.  However, the point of this exercise is to prove a simpler statement. Suppose someone gives a matrix $C$ and claims that $C = AB$, can we {\em quickly verify} if the claim is true?

\begin{parts}
\part[4] First prove a warm-up statement: suppose $a$ and $b$ are any two $0/1$ vectors of length $n$, and suppose that $a \ne b$.  Then, for a random binary vector $x \in \{0,1\}^n$ (one in which each coordinate is chosen uniformly at random), prove that $\Pr [ \langle a, x\rangle = \langle b, x\rangle~ (\text{mod } 2)] = 1/2$.
\part[6] Now, design an $O(n^2)$ time algorithm that tests if $C = AB$ and has a success probability $\ge 1/2$. (You need to prove the probability bound.)
\part[2] Show how to improve the success probability to $7/8$ while still having running time $O(n^2)$.
\end{parts}

\begin{parts}

\part If $a$ and $b$ can be any vectors $\in \{0,1\}^n$ and $x$ is also $\in \{0,1\}^n$ with each coordinate is chosen uniformly at random, then the probability of any term $a_i\times x_i$ or $b_i\times x_i$ where $0\leq i \leq n$ in the inner product being $0$ is $\frac{3}{4}$ corresponding to the pairs $\{00, 01, 10\}$. The probability of any term being $1$ is $1-\frac{3}{4}=\frac{1}{4}$.


I got the hint for the following recurrence from math.stackexchange.com. But I can't find the page now to list as a reference.

Let $a_N$ be the probability that a product of two random binary vectors has an even number of $1$'s. This happens when for the first term to be summed in the inner product is $0$ followed by an even number of $1$'s, or the other way round. We can see that $a_0=1$ and for $N\geq 1$

$$
a_N = p_0 a_{N-1} + p_1 (1-a_{N-1})
$$

where $p_0$ is the probability of a term in the dot product being $0$ and $p_1$ is the probability of a term in the dot product being $1$. The solution to this recurrence can be shown to be equal to the following by induction

$$
a_N = \frac{1}{2} + \frac{(p_0-p_1)^N}{2}
$$

For the base case where $N=0$, we get $a_0 = \frac{1}{2} + \frac{(p_0-p_1)^0}{2} = \frac{1}{2}  + \frac{1}{2} =1$. Let us assume that this is true for $N-1$. Then 

\begin{equation*}
\begin{aligned}
a_N &= p_0 \left ( \frac{1}{2} + \frac{(p_0-p_1)^{N-1}}{2} \right ) + p_1 \left (1-\left ( \frac{1}{2} + \frac{(p_0-p_1)^{N-1}}{2} \right ) \right )\\
&= \frac{p_0+p_1}{2} + \left ( p_0 - p_1 \right ) \frac{(p_0-p_1)^{N-1}}{2}\\
&= \frac{1}{2} + \frac{(p_0-p_1)^N}{2} \text{, which proves the recurrence}\\
&= \frac{1}{2} + \frac{(\frac{3}{4}-\frac{1}{4})^N}{2} \text{, by plugging in }p_0 =\frac{3}{4} \text { and }p_1=\frac{1}{4}\\
&= \frac{1}{2} + \frac{\frac{1}{2}^N}{2}\\
\Pr [ \langle a, x\rangle = \langle b, x\rangle~ (\text{mod } 2)] &= \Pr \left [ \text{both inner products terms have odd } 1 \text{'s}  \text{ or both have even } 1 \text{'s}  \right ]\\
&= a_Na_N + (1-a_N)(1-a_N)\\ 
&= a_N^2 + 1 -2a_N + a_N^2\\
&= 2a_N^2 -2a_N + 1 = \frac{1}{2} \text{, make this assumption for the time being}\\
\Rightarrow 2a_N^2 -2a_N +\frac{1}{2} &= 0\\
4a_N^2 -4a_N +1&=0\\
(2a_N-1)^2 &= 0\\
a_N &= \frac{1}{2} \text{, which cannot be true by } a_N \text{ given above}\\
\Rightarrow \Pr [ \langle a, x\rangle = \langle b, x\rangle~ (\text{mod } 2)] &\neq \frac{1}{2} 
\end{aligned}
\end{equation*}


\part Although I was not able to prove that $\Pr [ \langle a, x\rangle = \langle b, x\rangle~ (\text{mod } 2)] = 1/2$. I will use this to be a known fact for the purposes of this part.

Using Freivalds' algorithm \cite{matrix}, create a random binary vector $x \in \{0,1\}^n$ (one in which each coordinate is chosen uniformly at random). $A$ and $B$ are the two $n \times n$ matrices over the field $\mathbf{F}_2$. Evaluate $P=A \times \left ( B x \right ) - Cx$. If $P=\{0\}^n$, we output \textit{true} signifying that $AB=C$, else we output \textit{false}.


When $AB=C$, $\Pr \left [ P=\{0\}^n \right ] = 1$ since any value of $x$ will result in $P=A \times \left ( B x \right ) - Cx$.

When $AB \neq C$, for $P$ to be $\{0\}^n$, we would need every element in $A \times \left ( B x \right )$ to be the same as the corresponding element in $Cx$ mod $2$. Since $A \times \left ( B x \right ) = \left ( A \times B\right ) x  $, $\Pr \left [ P=\{0\}^n \right ] = \left (\frac{1}{2} \right ) ^n$, since every element in vector $\left ( A \times B\right ) x  $ would need to be the same as every element in vector $Cx$ mod $2$. Each of these will be equal with probability $\frac{1}{2}$ as per part a above. And for all of them to be equal, it will be $\left (\frac{1}{2} \right ) ^n$. So for the case when $n=2$, $\Pr \left [ P=\{0\}^n \right ] = \left (\frac{1}{2} \right ) ^n = \frac{1}{4}$ or in other words the success probability is $1-\frac{1}{4}=\frac{3}{4}$. In the general case the success probability is therefore $]\geq \frac{1}{2}$.

To compute vector $Bx$, we would need to do $n$ inner products and each inner product will take $n$ work. So time complexity will be $O(n^2)$. Then to compute $A \times \left ( B x \right )$, where we multiply matrix $A$ with vector $Bx$, the time complexity will again be $O(n^2)$. To compute $P=A \times \left ( B x \right ) - Cx$ after this, we would need to do $O(n^2)$ work to compute $Cx$ and then $n$ subtractions. So total time complexity is $O(n^2)$. 

\part To improve the success probability to $7/8$ while still having running time $O(n^2)$, repeat the algorithm described in part b three times and output \textit{true} signifying that $AB=C$ only if $P=\{0\}^n$ each of these three times. So for the algorithm to output true when $AB \neq C$, the probability would be $\leq \left ( \frac{1}{2} \right ) ^3 = \frac{1}{8}$. The success probability will be $\geq 1-\frac{1}{8}=\frac{7}{8}$. The time complexity will still be $O(n^2)$ even with repeating an $O(n^2)$ algorithm $3$ times.

\end{parts}

\titledquestion{Coloring graphs}
One of the popular problems in graph theory is that of graph coloring: we are given an undirected graph $G = (V, E)$, and the aim is to assign each vertex $u$ a {\em color} $c(u)$, with the guarantee that for any edge $u, v$, $c(u) \ne c(v)$.  (i.e., end-points of edges must receive different colors.) The goal is to color the graph using the fewest total number of colors, subject to the above condition.

Coloring is known to be a very hard problem. Suppose we are OK with something weaker: suppose that we consider an assignment of colors {\em acceptable} if $c(u) \ne c(v)$ holds for $\ge 90\%$ of the edges.

\begin{parts}
\part[7] Suppose that we {\em randomly} assign a color in the range $\{1, 2, \dots, 20\}$ to the vertices.  Prove that we obtain an acceptable assignment with probability $\ge 1/2$.  (Note that the algorithm is quite remarkable -- it doesn't even look at the edges of the graph!)
\part[3] What happens above when we randomly assign colors in the range $\{1, 2, \dots, 11\}$? Can we still obtain an algorithm that succeeds with probability $\ge 1/2$? 
\end{parts}

\begin{parts}

\part \begin{equation*}
\begin{aligned}
    \text{Let random variable } E_i &= 
\begin{dcases}
    1,& \text{when vertices at the two ends of edge } E_i \text{ are of different colors}\\
    0,              & \text{otherwise}
\end{dcases}\\
\text{Let } E_T \text{ be the number}&\text{ of edges where vertices at the two ends are of different colors}\\
\mathbb{E} \left [ E_T \right ] &= \mathbb{E} \left [ \sum_{i=1}^n E_i \right ] \text{, where } n \text{ is the number of vertices}\\
&= \sum_{i=1}^n \mathbb{E} \left [  E_i \right ] \text{, by linearity of Expectation}\\
&= \sum_{i=1}^n \left ( 1\times \Pr \left[ E_i = 1 \right ] + 0 \times \Pr \left[ E_i = 0 \right ] \right )\\
&= \sum_{i=1}^n \Pr \left[ E_i = 1 \right ]\\
&= \sum_{i=1}^n \frac{19}{20} \text{, as once a color is given to a vertex, one of } 19 \text{ can be given to the other}\\
&= \frac{19}{20}n\\
&= 0.95n
\end{aligned}
\end{equation*}

Since the expected value or mean of the number of edges where vertices at the two ends are of different colors is $ 0.95n$, the probability that the number of edges where vertices at the two ends are of different colors is $\geq 0.95n$ is $\frac{1}{2}$. This means that the probability that the number of edges where vertices at the two ends are of different colors is the $\geq 0.90n$ is definitely $\geq \frac{1}{2}$ since this covers more than half the probability mass. 

\part In the case where we randomly assign colors in the range $\{1, 2, \dots, 11\}$, $\mathbb{E} \left [ E_T \right ] = \frac{10}{11}n = 0.91n$ based on a computation similar to the one shown above. So the the probability that the number of edges where vertices at the two ends are of different colors is the $\geq 0.90n$ is definitely $\geq \frac{1}{2}$ since this covers more than half the probability mass.

\end{parts}

\end{questions}

\begin{thebibliography}{9}

\bibitem{matrix} \enquote{Freivalds' Algorithm.} \textit{Wikipedia}, Wikimedia Foundation, 30 Sept. 2018, 
\url{https://en.wikipedia.org/wiki/Freivalds\%27\_algorithm}.

\end{thebibliography}

\end{document}
